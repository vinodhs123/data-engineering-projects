{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /dbfs/FileStore/json/edp_bronze_json\n",
    "#dbutils.widgets.text(\"directory\", \"\")\n",
    "folder_path = dbutils.widgets.get(\"directory\")\n",
    "output_path = dbutils.widgets.get(\"DDL_file_path\")\n",
    "source_path_dev = dbutils.widgets.get(\"source_path_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "# dir_path = '/dbfs/FileStore/Json_Aug23'  #'/dbfs/FileStore/Json'\n",
    "ss_missing = []\n",
    "for file in os.listdir(folder_path):\n",
    "    filename = os.path.join(folder_path, file)\n",
    "    with open(filename, 'r') as data:\n",
    "        item = json.load(data)[0]\n",
    "        par_name = item[\"source_details\"][\"source_table\"].lower()\n",
    "        data_path = f\"\"\"{source_path_dev}{par_name}\"\"\"\n",
    "        # data_path = f\"\"\"/mnt/landing/Blckstrw/{par_name}\"\"\"\n",
    "        try:\n",
    "            df = spark.read.parquet(data_path)\n",
    "            schema = df.schema\n",
    "            schema_str = \", \".join([f\"{field.name}:{field.dataType.simpleString()}\" for field in schema])\n",
    "        # print(schema_str)\n",
    "            with open(f\"\"\"{output_path}{item[\"source_details\"][\"source_table\"].lower()}.ddl\"\"\", \"w\") as file:\n",
    "                file.write(schema_str)\n",
    "                print(f\">> DDL created for {filename}\")\n",
    "        except:\n",
    "            ss_missing.append(data_path)\n",
    "            continue\n",
    "\n",
    "\n",
    "print(\"/n missing: \", ss_missing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
